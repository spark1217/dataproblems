# Data Science Prep [ML]
### Information Gain & Entropy in Decision Tree  
Explain what information gain and entropy are in a decision tree.  
  
    
##### Information gain  
- A measure of how much information a feature provides about a class. It helps to determine the order of attributes in the nodes of a decision tree.  
- Entropy before split - Weighted entropy after split  
  
##### Entropy  
- A metric that measures the impurity or uncertainty in a group of observations. It determines how a decision tree chooses to split data.  
  
    
      
### Decision tree  
- Can perform lassification, regression, multioutput tasks  
- Powerful, Can fit complicated dataset  
- The fundamental components of random forests  
- Decision Tree requires very little data preparation  
- No feature scaling or centering are required  
- 'gini' attribute: measure its impurity. gini=0 then node is pure.  
- Gini impurity measure is used by default. Slightly faster to compute.    
- Entropy impurity measure can be selected. Produce slightly more balanced trees.    
- Entropy: a measure of molecular disorder. Approaches to 0 when molecules are well ordered and stable. 
- Information theory: measures the average information content of a message. Entropy is 0 when all messages are identical.  
